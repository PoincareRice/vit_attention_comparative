# ğŸ§  ViT Attention Comparative  
### Comparative Analysis of Advanced Attention Mechanisms in Vision Transformers  
**ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViT)ì—ì„œ ê³ ê¸‰ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜(Entmax, Sparsemax ë“±)ì„ ë¹„êµ ë¶„ì„í•˜ëŠ” í”„ë¡œì íŠ¸**

## ğŸ“˜ Overview | ê°œìš”

This repository provides a complete implementation for comparing **advanced attention mechanisms** (such as *Entmax* and *Sparsemax*) in **Vision Transformers (ViT)**.  
It supports training, evaluation, and visualization of attention maps on **CIFAR-10**.

ì´ ì €ì¥ì†ŒëŠ” **Vision Transformer(ViT)** ì—ì„œ **Entmax, Sparsemax ë“± ê³ ê¸‰ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜**ì„ ë¹„êµ ì‹¤í—˜í•˜ê¸° ìœ„í•œ ì „ì²´ êµ¬í˜„ì„ ì œê³µí•©ë‹ˆë‹¤.  
**CIFAR-10** ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ, í‰ê°€, ì–´í…ì…˜ ë§µ ì‹œê°í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

## ğŸ“‚ Project Structure | í”„ë¡œì íŠ¸ êµ¬ì¡°

```
VIT_ATTENTION_COMPARATIVE/
â”œâ”€â”€ bash_folder/               # Shell scripts for training & evaluation | í•™ìŠµ/í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ train.bash
â”‚   â””â”€â”€ evaluate.bash
â”œâ”€â”€ configs/                   # Experiment and model configuration files | ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ experiment1.yaml
â”œâ”€â”€ data/                      # Dataset storage (CIFAR-10) | ë°ì´í„° ì €ì¥ ê²½ë¡œ
â”‚   â”œâ”€â”€ external/              # Raw/external data | ì›ë³¸ ë°ì´í„°
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ experiments/               # Experiment results | ì‹¤í—˜ ê²°ê³¼ ì €ì¥
â”‚   â””â”€â”€ exp_001/
â”‚       â”œâ”€â”€ checkpoints/       # Model checkpoints | ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
â”‚       â”œâ”€â”€ logs/
â”‚       â””â”€â”€ results/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                  # Dataset loaders | ë°ì´í„°ì…‹ ë¡œë”
â”‚   â”œâ”€â”€ evaluation/            # Evaluation scripts | í‰ê°€ ì½”ë“œ
â”‚   â”œâ”€â”€ models/                # Model definitions | ëª¨ë¸ ì •ì˜
â”‚   â”‚   â”œâ”€â”€ attention/
â”‚   â”‚   â”‚   â”œâ”€â”€ base_attention.py
â”‚   â”‚   â”‚   â”œâ”€â”€ entmax_attention.py
â”‚   â”‚   â”‚   â””â”€â”€ sparsemax_attention.py
â”‚   â”‚   â””â”€â”€ vit_custom.py
â”‚   â”œâ”€â”€ training/              # Training scripts | í•™ìŠµ ì½”ë“œ
â”‚   â””â”€â”€ utils/                 # Utility functions | ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ tests/                     # Unit tests | í…ŒìŠ¤íŠ¸ ì½”ë“œ
â””â”€â”€ requirements.txt
```

## âš™ï¸ Installation | ì„¤ì¹˜ ë°©ë²•

```bash
# Clone repository | ì €ì¥ì†Œ ë³µì œ
git clone https://github.com/PoincareRice/vit_attention_comparative.git
cd vit_attention_comparative

# Create virtual environment (optional) | ê°€ìƒí™˜ê²½ ìƒì„± (ì„ íƒ)
conda create -n vit-env python=3.10 -y
conda activate vit-env
#ubuntu22.04
python3.10 -m venv vit-env
source ./vit-env/bin/activate

# Install dependencies | í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

## ğŸš€ Usage | ì‚¬ìš© ë°©ë²•

### Training | í•™ìŠµ ì‹¤í–‰
```bash
bash ./bash_folder/train.bash
```
Trains a ViT model using the selected attention mechanism (softmax, entmax, or sparsemax).
Saves checkpoints under experiments/<exp_name>/checkpoints/.

ì„ íƒí•œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜(softmax, entmax, sparsemax)ìœ¼ë¡œ ViT ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
ê²°ê³¼ ê°€ì¤‘ì¹˜ëŠ” experiments/<exp_name>/checkpoints/ í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.

### Evaluation | í‰ê°€ ì‹¤í–‰
```bash
bash ./bash_folder/evaluate.bash
```
Evaluates a saved checkpoint and visualizes attention maps.
Results are stored under experiments/<exp_name>/results/.

ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì™€ ëª¨ë¸ì„ í‰ê°€í•˜ê³  ì–´í…ì…˜ ë§µì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
ê²°ê³¼ëŠ” experiments/<exp_name>/results/ í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.

## ğŸ§© Configuration | ì„¤ì • íŒŒì¼ ì˜ˆì‹œ
```yaml
model:
  base_model: "google/vit-base-patch16-224"
  attention_type: "entmax"      # options: softmax, entmax, sparsemax
  num_labels: 10

training:
  epochs: 10
  learning_rate: 3e-5
  weight_decay: 0.01
  device: "cuda"

data:
  dataset: "CIFAR10"
  image_size: 224
  batch_size: 32
  num_workers: 4

experiment:
  exp_name: "exp_001"
  checkpoint_file: "experiments/exp_001/checkpoints/vit_entmax_epoch10.pth"
```

## ğŸ§  Attention Mechanisms | ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì¢…ë¥˜
| Type | Description | ì„¤ëª… |
|---|---|---|
| Softmax | Standard attention used in the original Transformer. | ê¸°ë³¸ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì‚¬ìš©ëœ í‘œì¤€ ì–´í…ì…˜ ë°©ì‹ |
| Entmax | Introduces controlled sparsity for better interpretability. | í¬ì†Œì„±ì„ ë¶€ì—¬í•˜ì—¬ ì£¼ì˜ ì§‘ì¤‘ì„ ë” ëª…í™•íˆ í•¨ |
| Sparsemax	| Produces fully sparse attention distributions. | ì™„ì „ í¬ì†Œí•œ ì£¼ì˜ ë¶„í¬ë¥¼ ìƒì„±í•¨ |

## ğŸ§‘â€ğŸ’» Citation | ì¸ìš© ì •ë³´

If you use this project in your research, please cite:
ì´ í”„ë¡œì íŠ¸ë¥¼ ì—°êµ¬ì— í™œìš©í•˜ì‹ ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì¸ìš©í•´ ì£¼ì„¸ìš”.
```latex
@misc{vit_attention_comparative_2025,
  author = {Jeon, Seongyoon, Lee, Jaewon and Park, Geumrin},
  title = {A Comparative Study of Advanced Attention Mechanisms in Vision Transformers},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/<your-username>/VIT_ATTENTION_COMPARATIVE}}
}
```

## ğŸ§© To-Do | í–¥í›„ ê³„íš

- **Expand dataset to ImageNet**  
  Currently, CIFAR-10 is used for quick verification, but future experiments will extend to the larger and more complex ImageNet dataset to evaluate generalization performance.
  í˜„ì¬ëŠ” ë¹ ë¥¸ ê²°ê³¼ í™•ì¸ì„ ìœ„í•´ CIFAR-10 ë°ì´í„°ì…‹ì„ ì‚¬ìš©í–ˆì§€ë§Œ, í–¥í›„ì—ëŠ” ë” ë³µì¡í•œ ImageNet ë°ì´í„°ì…‹ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ê²€ì¦í•  ì˜ˆì •ì…ë‹ˆë‹¤.

- **Explore additional advanced attention mechanisms**  
  Plan to implement and compare other improved attention mechanisms such as Linear Attention, Performer, and Reformer.
  Linear Attention, Performer, Reformer ë“± ë‹¤ì–‘í•œ ì–´í…ì…˜ ê°œì„  ê¸°ë²•ì„ ì¶”ê°€ë¡œ êµ¬í˜„í•˜ê³  ë¹„êµ ì‹¤í—˜í•  ì˜ˆì •ì…ë‹ˆë‹¤.

- **Implement Î±-Entmax parameter tuning**  
  Optimize Î± values to analyze the trade-off between sparsity and interpretability.
  í¬ì†Œì„±ê³¼ í•´ì„ ê°€ëŠ¥ì„± ê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´ Î± ê°’ì„ ë¯¸ì„¸ ì¡°ì •í•  ì˜ˆì •ì…ë‹ˆë‹¤.

- **Integrate Grad-CAM for interpretability**  
  Visualize which image regions the model attends to under different attention mechanisms.
  ê° ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì—ì„œ ëª¨ë¸ì´ ì£¼ëª©í•˜ëŠ” ì´ë¯¸ì§€ ì˜ì—­ì„ ì‹œê°í™”í•˜ê¸° ìœ„í•´ Grad-CAMì„ í†µí•©í•  ì˜ˆì •ì…ë‹ˆë‹¤.